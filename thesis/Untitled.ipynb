{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/giangvdq/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/giangvdq/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "def imports():\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.models.ldamodel import LdaModel\n",
    "    from gensim.models import CoherenceModel\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    from datetime import datetime\n",
    "    \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import math\n",
    "    import numpy as np # linear algebra\n",
    "    import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "    import os\n",
    "    \n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "    import contractions \n",
    "    nltk.download('wordnet')\n",
    "    #CONFIG\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    input_path = '/home/giangvdq/data/NIPS Papers'\n",
    "    os.chdir(input_path)\n",
    "    \n",
    "imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "\n",
    "papers = pd.read_csv('papers.csv')\n",
    "authors2 = pd.read_csv('authors.csv')\n",
    "paper_authors = pd.read_csv('paper_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, tweet_col):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # drop rows with empty values\n",
    "    df_copy.dropna(inplace=True)\n",
    "    \n",
    "    # format the date\n",
    "    df_copy[date_col] = df_copy[date_col].apply(lambda row: datetime.strptime(row, '%m-%d-%Y %H:%M:%S'))\n",
    "    \n",
    "    # filter rows older than a given date\n",
    "    df_copy = df_copy[df_copy[date_col] >=start_datetime]\n",
    "    \n",
    "    # lower the tweets\n",
    "    df_copy['preprocessed_' + tweet_col] = df_copy[tweet_col].str.lower()\n",
    "    \n",
    "    # filter out stop words and URLs\n",
    "    en_stop_words = set(stopwords.words('english'))\n",
    "    extended_stop_words = en_stop_words | \\\n",
    "                        {\n",
    "                            '&amp;', 'rt',                           \n",
    "                            'th','co', 're', 've', 'kim', 'daca'\n",
    "                        }\n",
    "    url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'        \n",
    "    df_copy['preprocessed_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: ' '.join([word for word in row.split() if (not word in stop_words) and (not re.match(url_re, word))]))\n",
    "    \n",
    "    # tokenize the tweets\n",
    "    tokenizer = RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*')\n",
    "    df_copy['tokenized_' + tweet_col] = df_copy['preprocessed_' + tweet_col].apply(lambda row: tokenizer.tokenize(row))\n",
    "    \n",
    "    df_copy = preprocess2(df_copy)\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = papers.head(10)\n",
    "df_copy = df.copy()\n",
    "\n",
    "# lower the tweets\n",
    "df_copy['col2'] = df_copy['paper_text'].str.lower()\n",
    "\n",
    "# replace \\n with space\n",
    "df_copy = df_copy.replace(to_replace='\\n',value=' ',regex=True)\n",
    "\n",
    "# expand contractions\n",
    "df_copy['col2'] = df_copy['col2'].apply(lambda x: decontracted(x))\n",
    "\n",
    "# filter out stop words and URLs\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "extended_stop_words = en_stop_words | \\\n",
    "                {\n",
    "                        '&amp;', 'rt',                           \n",
    "                        'th','co', 're', 've', 'kim', 'daca'\n",
    "                }\n",
    "url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'        \n",
    "df_copy['col2'] = df_copy['col2'].apply(lambda row: ' '.join([word for word in row.split() if (not word in extended_stop_words) and (not re.match(url_re, word))]))\n",
    "\n",
    "\n",
    "df_copy['col2'] = df_copy['col2'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
    "df_copy['col2'] = df_copy['col2'].str.replace(r'\\b\\w\\b', '').str.replace(r'\\s+', ' ')\n",
    "\n",
    "# tokenize the tweets\n",
    "# tokenizer = RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*')\n",
    "# df_copy['col3'] = df_copy['col2'].apply(lambda row: tokenizer.tokenize(row))\n",
    "# Use English stemmer.\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "# df_copy['col3'] = df_copy['col3'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "df['d3'] = df_copy['col2'].apply(lemmatize_text)\n",
    "\n",
    "df_copy['col3'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
